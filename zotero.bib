
@inproceedings{prochazka_which_2023,
	location = {Tatranské Matliare, Slovakia},
	title = {Which Graph Properties Affect {GNN} Performance for a Given Downstream Task?},
	volume = {3498},
	rights = {All rights reserved},
	url = {https://ceur-ws.org/Vol-3498/#paper7},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Machine learning algorithms on graphs, in particular graph neural networks, became a popular framework for solving various tasks on graphs, attracting significant interest in the research community in recent years. As presented, however, these algorithms usually assume that the input graph is fixed and well-defined and do not consider the problem of constructing the graph for a given practical task. This work proposes a methodical way of linking graph properties with the performance of a {GNN} solving a given task on such graph via a surrogate regression model that is trained to predict the performance of the {GNN} from the properties of the graph dataset. Furthermore, the {GNN} model hyper-parameters are optionally added as additional features of the surrogate model and it is shown that this technique can be used to solve the practical problem of hyper-parameter tuning. We experimentally evaluate the importance of graph properties as features of the surrogate model with regards to the node classification task for several common graph datasets and discuss how these results can be used for graph composition tailored to the given task. Finally, our experiments indicate a significant gain in the proposed hyper-parameter tuning method compared to the reference grid-search method.},
	eventtitle = {Information Technologies – Applications and Theory 2023},
	pages = {58--66},
	booktitle = {Proceedings of the 23nd Conference Information Technologies – Applications and Theory ({ITAT} 2023)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2023-09-30},
	date = {2023-10-07},
	langid = {english},
}

@inproceedings{dedic_balancing_2023,
	location = {Torino, Italy},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://mlg-europe.github.io/papers/234.pdf},
	abstract = {Abstract Graph based models are used for tasks with increasing size and computational demands. We present a method for node classification that allows a user to precisely select the resolution at which the graph in question should be pretrained. Our method builds on an existing algorithm for pretraining on coarser graphs, {HARP}, which we extend in order to tune the effect of graph coarsening on the accuracy of node classification on a fine level. We present a novel way of refining the reduced graph in a targeted way based on the node classification confidence of particular nodes. This enhancement provides sufficient detail where needed, while collapsing structures where per-node information is not necessary for sufficient node classification accuracy. Hence, the method provides a meta-model for enhancing graph embedding models such as node2vec. We apply it to several datasets and discuss the differing behaviour on each of them in the context of their properties.},
	eventtitle = {20th International Workshop on Mining and Learning with Graphs},
	author = {Dědič, Marek and Bajer, Lukáš and Procházka, Pavel and Holeňa, Martin},
	urldate = {2023-09-22},
	date = {2023-09-22},
	langid = {english},
}

@inproceedings{prochazka_scalable_2022,
	location = {Zuberec, Slovakia},
	title = {Scalable Graph Size Reduction for Efficient {GNN} Application},
	volume = {3226},
	rights = {All rights reserved},
	url = {http://ceur-ws.org/Vol-3226/#paper9},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Graph neural networks ({GNN}) present a dominant framework for representation learning on graphs for the past several years. The main strength of {GNNs} lies in the fact that they can simultaneously learn from both node related attributes and relations between nodes, represented by edges. In tasks leading to large graphs, {GNN} often requires significant computational resources to achieve its superior performance. In order to reduce the computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple scalable task-aware graph preprocessing procedure allowing us to obtain a reduced graph such as {GNN} achieves a given desired performance on the downstream task. In addition, the proposed preprocessing allows for fitting the reduced graph and {GNN} into a given memory/computational resources. The proposed preprocessing is evaluated and compared with several reference scenarios on conventional {GNN} benchmark datasets.},
	eventtitle = {Information Technologies – Applications and Theory 2022},
	pages = {75--84},
	booktitle = {Proceedings of the 22nd Conference Information Technologies – Applications and Theory ({ITAT} 2022)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2022-09-30},
	date = {2022-09-30},
	langid = {english},
}

@inproceedings{grover_node2vec_2016,
	title = {node2vec: Scalable feature learning for networks},
	shorttitle = {node2vec},
	pages = {855--864},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Grover, Aditya and Leskovec, Jure},
	date = {2016},
	file = {Full Text:/home/madedic/Zotero/storage/Y8U8YZQI/PMC5108654.html:text/html;Snapshot:/home/madedic/Zotero/storage/ITX5DJ45/2939672.html:text/html},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: Online learning of social representations},
	shorttitle = {Deepwalk},
	pages = {701--710},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	date = {2014},
	file = {Full Text:/home/madedic/Zotero/storage/ISN68YG2/Perozzi et al. - 2014 - Deepwalk Online learning of social representation.pdf:application/pdf;Snapshot:/home/madedic/Zotero/storage/Y3TWLT7L/2623330.html:text/html},
}

@inproceedings{kipf_semi-supervised_2017,
	location = {Toulon, France},
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	url = {https://openreview.net/forum?id=SJU4ayYgl},
	abstract = {Semi-supervised classification with a {CNN} model for graphs. State-of-the-art results on a number of citation network datasets.},
	eventtitle = {International Conference on Learning Representations ({ICLR})},
	booktitle = {5th International Conference on Learning Representations, \{{ICLR}\} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher = {{OpenReview}.net},
	author = {Kipf, Thomas N. and Welling, Max},
	date = {2017-04-24},
	langid = {english},
}

@article{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {https://openreview.net/forum?id=idpCdOWtqXd60},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity...},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2020-11-03},
	date = {2013-01-17},
	langid = {english},
	file = {Snapshot:/home/madedic/Zotero/storage/KERQU498/forum.html:text/html},
}

@online{kubara_machine_2020,
	title = {Machine Learning Tasks on Graphs},
	url = {https://towardsdatascience.com/machine-learning-tasks-on-graphs-7bc8f175119a},
	abstract = {Can We Divide It Into Supervised/Unsupervised Learning? It’s Not That Simple…},
	titleaddon = {Medium},
	author = {Kubara, Kacper},
	urldate = {2020-11-03},
	date = {2020-09-28},
	langid = {english},
	file = {Snapshot:/home/madedic/Zotero/storage/4IYMYT34/machine-learning-tasks-on-graphs-7bc8f175119a.html:text/html},
}

@inproceedings{gori_new_2005,
	title = {A new model for learning in graph domains},
	volume = {2},
	doi = {10.1109/IJCNN.2005.1555942},
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network ({GNN}), capable of directly processing graphs. {GNNs} extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for {GNNs} is proposed and some experiments are discussed which assess the properties of the model.},
	eventtitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	pages = {729--734 vol. 2},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	date = {2005-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Neural networks, Machine learning, Machine learning algorithms, Application software, data structures, Data structures, Encoding, Focusing, graph neural network, graph theory, graphical data structures, learning (artificial intelligence), learning algorithm, neural nets, Recurrent neural networks, recursive neural networks, Software engineering, Tree graphs},
	file = {IEEE Xplore Abstract Record:/home/madedic/Zotero/storage/33929VKF/1555942.html:text/html},
}

@inproceedings{dedic_balancing_2024,
	location = {Vienna, Austria},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://openreview.net/forum?id=DrHwIzz93C},
	abstract = {We present a method for graph node classification that allows a user to precisely select the resolution at which the graph in question should be simplified and through this provides a way of choosing a suitable point in the performance-complexity trade-off. The method is based on refining a reduced graph in a targeted way following the node classification confidence for particular nodes.},
	eventtitle = {The Twelfth International Conference on Learning Representations},
	booktitle = {The Second Tiny Papers Track at {ICLR} 2024},
	author = {Dědič, Marek and Bajer, Lukas and Prochazka, Pavel and Holena, Martin},
	urldate = {2024-04-18},
	date = {2024-05-11},
	langid = {english},
}

@book{vapnik_nature_1995,
	location = {New York, {NY}},
	title = {The Nature of Statistical Learning Theory},
	rights = {http://www.springer.com/tdm},
	isbn = {978-1-4757-2440-0},
	url = {http://link.springer.com/10.1007/978-1-4757-2440-0},
	publisher = {Springer},
	author = {Vapnik, Vladimir N.},
	urldate = {2024-08-24},
	date = {1995},
	langid = {english},
	doi = {10.1007/978-1-4757-2440-0},
	keywords = {algorithms, boundary element method, construction, controlling, convergence, function, functional, learning, learning algorithm, learning theory, model, proof, Statistica, statistical theory, statistics},
	file = {Full Text:/home/madedic/Zotero/storage/3D37FJ2B/Vapnik - 1995 - The Nature of Statistical Learning Theory.pdf:application/pdf},
}

@inproceedings{gilmer_neural_2017,
	location = {Sydney, {NSW}, Australia},
	title = {Neural message passing for Quantum chemistry},
	series = {{ICML}'17},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks ({MPNNs}) and explore additional novel variations within this framework. Using {MPNNs} we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	pages = {1263--1272},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	publisher = {{JMLR}.org},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	urldate = {2024-08-26},
	date = {2017-08-06},
	file = {Full Text PDF:/home/madedic/Zotero/storage/KM28LB45/Gilmer et al. - 2017 - Neural message passing for Quantum chemistry.pdf:application/pdf},
}
