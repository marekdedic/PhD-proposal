
@inproceedings{prochazka_which_2023,
	location = {Tatranské Matliare, Slovakia},
	title = {Which Graph Properties Affect {GNN} Performance for a Given Downstream Task?},
	volume = {3498},
	rights = {All rights reserved},
	url = {https://ceur-ws.org/Vol-3498/#paper7},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Machine learning algorithms on graphs, in particular graph neural networks, became a popular framework for solving various tasks on graphs, attracting significant interest in the research community in recent years. As presented, however, these algorithms usually assume that the input graph is fixed and well-defined and do not consider the problem of constructing the graph for a given practical task. This work proposes a methodical way of linking graph properties with the performance of a {GNN} solving a given task on such graph via a surrogate regression model that is trained to predict the performance of the {GNN} from the properties of the graph dataset. Furthermore, the {GNN} model hyper-parameters are optionally added as additional features of the surrogate model and it is shown that this technique can be used to solve the practical problem of hyper-parameter tuning. We experimentally evaluate the importance of graph properties as features of the surrogate model with regards to the node classification task for several common graph datasets and discuss how these results can be used for graph composition tailored to the given task. Finally, our experiments indicate a significant gain in the proposed hyper-parameter tuning method compared to the reference grid-search method.},
	eventtitle = {Information Technologies – Applications and Theory 2023},
	pages = {58--66},
	booktitle = {Proceedings of the 23nd Conference Information Technologies – Applications and Theory ({ITAT} 2023)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2023-09-30},
	date = {2023-10-07},
	langid = {english},
}

@inproceedings{dedic_balancing_2023,
	location = {Torino, Italy},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://mlg-europe.github.io/papers/234.pdf},
	abstract = {Abstract Graph based models are used for tasks with increasing size and computational demands. We present a method for node classification that allows a user to precisely select the resolution at which the graph in question should be pretrained. Our method builds on an existing algorithm for pretraining on coarser graphs, {HARP}, which we extend in order to tune the effect of graph coarsening on the accuracy of node classification on a fine level. We present a novel way of refining the reduced graph in a targeted way based on the node classification confidence of particular nodes. This enhancement provides sufficient detail where needed, while collapsing structures where per-node information is not necessary for sufficient node classification accuracy. Hence, the method provides a meta-model for enhancing graph embedding models such as node2vec. We apply it to several datasets and discuss the differing behaviour on each of them in the context of their properties.},
	eventtitle = {20th International Workshop on Mining and Learning with Graphs},
	author = {Dědič, Marek and Bajer, Lukáš and Procházka, Pavel and Holeňa, Martin},
	urldate = {2023-09-22},
	date = {2023-09-22},
	langid = {english},
}

@article{yuan_explainability_2022,
	title = {Explainability in graph neural networks: A taxonomic survey},
	volume = {45},
	shorttitle = {Explainability in graph neural networks},
	pages = {5782--5799},
	number = {5},
	journaltitle = {{IEEE} transactions on pattern analysis and machine intelligence},
	author = {Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
	date = {2022},
	note = {Publisher: {IEEE}},
}

@inproceedings{ying_gnnexplainer_2019,
	title = {{GNNExplainer}: Generating Explanations for Graph Neural Networks},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html},
	shorttitle = {{GNNExplainer}},
	abstract = {Graph Neural Networks ({GNNs}) are a powerful tool for machine learning on graphs.{GNNs} combine node feature information with the graph structure by 
recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models,
and explaining predictions made by {GNNs} remains unsolved. Here
we propose {GNNExplainer}, the first general, model-agnostic approach for providing interpretable explanations for predictions of any {GNN}-based model on any graph-based machine learning task. Given an instance, {GNNExplainer} identifies a compact subgraph structure and a small subset of node features that have a crucial role in {GNN}'s prediction. 
Further, {GNNExplainer}  can generate consistent and concise explanations for an entire class of instances.
We formulate {GNNExplainer} as an optimization task that maximizes the mutual information between a {GNN}'s prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1\% on average. {GNNExplainer}  provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty {GNNs}.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
	urldate = {2023-08-30},
	date = {2019},
}

@inproceedings{prochazka_scalable_2022,
	location = {Zuberec, Slovakia},
	title = {Scalable Graph Size Reduction for Efficient {GNN} Application},
	volume = {3226},
	rights = {All rights reserved},
	url = {http://ceur-ws.org/Vol-3226/#paper9},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Graph neural networks ({GNN}) present a dominant framework for representation learning on graphs for the past several years. The main strength of {GNNs} lies in the fact that they can simultaneously learn from both node related attributes and relations between nodes, represented by edges. In tasks leading to large graphs, {GNN} often requires significant computational resources to achieve its superior performance. In order to reduce the computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple scalable task-aware graph preprocessing procedure allowing us to obtain a reduced graph such as {GNN} achieves a given desired performance on the downstream task. In addition, the proposed preprocessing allows for fitting the reduced graph and {GNN} into a given memory/computational resources. The proposed preprocessing is evaluated and compared with several reference scenarios on conventional {GNN} benchmark datasets.},
	eventtitle = {Information Technologies – Applications and Theory 2022},
	pages = {75--84},
	booktitle = {Proceedings of the 22nd Conference Information Technologies – Applications and Theory ({ITAT} 2022)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2022-09-30},
	date = {2022-09-30},
	langid = {english},
}

@inproceedings{grover_node2vec_2016,
	title = {node2vec: Scalable feature learning for networks},
	shorttitle = {node2vec},
	pages = {855--864},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Grover, Aditya and Leskovec, Jure},
	date = {2016},
	file = {Full Text:/home/madedic/Zotero/storage/Y8U8YZQI/PMC5108654.html:text/html;Snapshot:/home/madedic/Zotero/storage/ITX5DJ45/2939672.html:text/html},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: Online learning of social representations},
	shorttitle = {Deepwalk},
	pages = {701--710},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	date = {2014},
	file = {Full Text:/home/madedic/Zotero/storage/ISN68YG2/Perozzi et al. - 2014 - Deepwalk Online learning of social representation.pdf:application/pdf;Snapshot:/home/madedic/Zotero/storage/Y3TWLT7L/2623330.html:text/html},
}

@inproceedings{kipf_semi-supervised_2017,
	location = {Toulon, France},
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	url = {https://openreview.net/forum?id=SJU4ayYgl},
	abstract = {Semi-supervised classification with a {CNN} model for graphs. State-of-the-art results on a number of citation network datasets.},
	eventtitle = {International Conference on Learning Representations ({ICLR})},
	booktitle = {5th International Conference on Learning Representations, \{{ICLR}\} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher = {{OpenReview}.net},
	author = {Kipf, Thomas N. and Welling, Max},
	date = {2017-04-24},
	langid = {english},
}

@inproceedings{hamilton_inductive_2017,
	title = {Inductive representation learning on large graphs},
	pages = {1024--1034},
	booktitle = {Advances in neural information processing systems},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	date = {2017},
	file = {Full Text:/home/madedic/Zotero/storage/5Y9Q2N8U/Hamilton et al. - 2017 - Inductive representation learning on large graphs.pdf:application/pdf;Snapshot:/home/madedic/Zotero/storage/B8EPL4ZN/6703-inductive-representation-learning-on-large-graphs.html:text/html},
}

@article{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {https://openreview.net/forum?id=idpCdOWtqXd60},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity...},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2020-11-03},
	date = {2013-01-17},
	langid = {english},
	file = {Snapshot:/home/madedic/Zotero/storage/KERQU498/forum.html:text/html},
}

@online{kubara_machine_2020,
	title = {Machine Learning Tasks on Graphs},
	url = {https://towardsdatascience.com/machine-learning-tasks-on-graphs-7bc8f175119a},
	abstract = {Can We Divide It Into Supervised/Unsupervised Learning? It’s Not That Simple…},
	titleaddon = {Medium},
	author = {Kubara, Kacper},
	urldate = {2020-11-03},
	date = {2020-09-28},
	langid = {english},
	file = {Snapshot:/home/madedic/Zotero/storage/4IYMYT34/machine-learning-tasks-on-graphs-7bc8f175119a.html:text/html},
}

@inproceedings{gori_new_2005,
	title = {A new model for learning in graph domains},
	volume = {2},
	doi = {10.1109/IJCNN.2005.1555942},
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network ({GNN}), capable of directly processing graphs. {GNNs} extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for {GNNs} is proposed and some experiments are discussed which assess the properties of the model.},
	eventtitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	pages = {729--734 vol. 2},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	date = {2005-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Neural networks, Machine learning, Machine learning algorithms, Application software, data structures, Data structures, Encoding, Focusing, graph neural network, graph theory, graphical data structures, learning (artificial intelligence), learning algorithm, neural nets, Recurrent neural networks, recursive neural networks, Software engineering, Tree graphs},
	file = {IEEE Xplore Abstract Record:/home/madedic/Zotero/storage/33929VKF/1555942.html:text/html},
}

@article{dietterich_solving_1997,
	title = {Solving the multiple instance problem with axis-parallel rectangles},
	volume = {89},
	issn = {0004-3702},
	doi = {10.1016/S0004-3702(96)00034-3},
	abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
	pages = {31--71},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Dietterich, Thomas G. and Lathrop, Richard H. and Lozano-Pérez, Tomás},
	urldate = {2017-05-31},
	date = {1997-01-01},
	keywords = {Drug design, Machine learning, Structure-activity relationships},
}

@inproceedings{dedic_balancing_2024,
	location = {Vienna, Austria},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://openreview.net/forum?id=DrHwIzz93C},
	abstract = {We present a method for graph node classification that allows a user to precisely select the resolution at which the graph in question should be simplified and through this provides a way of choosing a suitable point in the performance-complexity trade-off. The method is based on refining a reduced graph in a targeted way following the node classification confidence for particular nodes.},
	eventtitle = {The Twelfth International Conference on Learning Representations},
	booktitle = {The Second Tiny Papers Track at {ICLR} 2024},
	author = {Dědič, Marek and Bajer, Lukas and Prochazka, Pavel and Holena, Martin},
	urldate = {2024-04-18},
	date = {2024-05-11},
	langid = {english},
}

@book{vapnik_nature_1995,
	location = {New York, {NY}},
	title = {The Nature of Statistical Learning Theory},
	rights = {http://www.springer.com/tdm},
	isbn = {978-1-4757-2440-0},
	url = {http://link.springer.com/10.1007/978-1-4757-2440-0},
	publisher = {Springer},
	author = {Vapnik, Vladimir N.},
	urldate = {2024-08-24},
	date = {1995},
	langid = {english},
	doi = {10.1007/978-1-4757-2440-0},
	keywords = {algorithms, boundary element method, construction, controlling, convergence, function, functional, learning, learning algorithm, learning theory, model, proof, Statistica, statistical theory, statistics},
	file = {Full Text:/home/madedic/Zotero/storage/3D37FJ2B/Vapnik - 1995 - The Nature of Statistical Learning Theory.pdf:application/pdf},
}

@inproceedings{gilmer_neural_2017,
	location = {Sydney, {NSW}, Australia},
	title = {Neural message passing for Quantum chemistry},
	series = {{ICML}'17},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks ({MPNNs}) and explore additional novel variations within this framework. Using {MPNNs} we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	pages = {1263--1272},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	publisher = {{JMLR}.org},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	urldate = {2024-08-26},
	date = {2017-08-06},
	file = {Full Text PDF:/home/madedic/Zotero/storage/KM28LB45/Gilmer et al. - 2017 - Neural message passing for Quantum chemistry.pdf:application/pdf},
}

@inproceedings{selvaraju_grad-cam_2017,
	title = {Grad-{CAM}: Visual Explanations From Deep Networks via Gradient-Based Localization},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html},
	shorttitle = {Grad-{CAM}},
	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
	pages = {618--626},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2024-08-26},
	date = {2017},
	file = {Full Text PDF:/home/madedic/Zotero/storage/LZ2R4K73/Selvaraju et al. - 2017 - Grad-CAM Visual Explanations From Deep Networks via Gradient-Based Localization.pdf:application/pdf},
}

@article{huang_graphlime_2023,
	title = {{GraphLIME}: Local Interpretable Model Explanations for Graph Neural Networks},
	volume = {35},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/9811416},
	doi = {10.1109/TKDE.2022.3187455},
	shorttitle = {{GraphLIME}},
	abstract = {Recently, graph neural networks ({GNN}) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. However, explaining the effectiveness of {GNN} models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose {GraphLIME}, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion ({HSIC}) Lasso, which is a nonlinear feature selection method. {GraphLIME} is a generic {GNN}-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. Through experiments on two real-world datasets, the explanations of {GraphLIME} are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.},
	pages = {6968--6972},
	number = {7},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Huang, Qiang and Yamada, Makoto and Tian, Yuan and Singh, Dinesh and Chang, Yi},
	urldate = {2024-08-26},
	date = {2023-07},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Computational modeling, Data models, explanation, Feature extraction, Graph neural networks, interpretability, Kernel, Mathematical models, Predictive models, Toy manufacturing industry},
	file = {IEEE Xplore Abstract Record:/home/madedic/Zotero/storage/9YUZMA5I/9811416.html:text/html;Submitted Version:/home/madedic/Zotero/storage/W5HC4JHC/Huang et al. - 2023 - GraphLIME Local Interpretable Model Explanations for Graph Neural Networks.pdf:application/pdf},
}

@inproceedings{ribeiro_why_2016,
	location = {San Francisco California {USA}},
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	shorttitle = {"Why Should I Trust You?},
	eventtitle = {{KDD} '16: The 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {1135--1144},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2024-08-26},
	date = {2016-08-13},
	langid = {english},
}

@article{yamada_high-dimensional_2014,
	title = {High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso},
	volume = {26},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00537},
	doi = {10.1162/NECO_a_00537},
	abstract = {The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this letter, we consider a feature-wise kernelized Lasso for capturing nonlinear input-output dependency. We first show that with particular choices of kernel functions, nonredundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures such as the Hilbert-Schmidt independence criterion. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments for classification and regression with thousands of features.},
	pages = {185--207},
	number = {1},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Yamada, Makoto and Jitkrittum, Wittawat and Sigal, Leonid and Xing, Eric P. and Sugiyama, Masashi},
	urldate = {2024-08-26},
	date = {2014-01-01},
	file = {Snapshot:/home/madedic/Zotero/storage/TW3RMD6Q/High-Dimensional-Feature-Selection-by-Feature-Wise.html:text/html;Submitted Version:/home/madedic/Zotero/storage/UBVDAB6D/Yamada et al. - 2014 - High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso.pdf:application/pdf},
}
