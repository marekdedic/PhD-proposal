
@inproceedings{prochazka_which_2023,
	location = {Tatranské Matliare, Slovakia},
	title = {Which Graph Properties Affect {GNN} Performance for a Given Downstream Task?},
	volume = {3498},
	rights = {All rights reserved},
	url = {https://ceur-ws.org/Vol-3498/#paper7},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Machine learning algorithms on graphs, in particular graph neural networks, became a popular framework for solving various tasks on graphs, attracting significant interest in the research community in recent years. As presented, however, these algorithms usually assume that the input graph is fixed and well-defined and do not consider the problem of constructing the graph for a given practical task. This work proposes a methodical way of linking graph properties with the performance of a {GNN} solving a given task on such graph via a surrogate regression model that is trained to predict the performance of the {GNN} from the properties of the graph dataset. Furthermore, the {GNN} model hyper-parameters are optionally added as additional features of the surrogate model and it is shown that this technique can be used to solve the practical problem of hyper-parameter tuning. We experimentally evaluate the importance of graph properties as features of the surrogate model with regards to the node classification task for several common graph datasets and discuss how these results can be used for graph composition tailored to the given task. Finally, our experiments indicate a significant gain in the proposed hyper-parameter tuning method compared to the reference grid-search method.},
	eventtitle = {Information Technologies – Applications and Theory 2023},
	pages = {58--66},
	booktitle = {Proceedings of the 23nd Conference Information Technologies – Applications and Theory ({ITAT} 2023)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2023-09-30},
	date = {2023-10-07},
	langid = {english},
}

@inproceedings{dedic_balancing_2023,
	location = {Torino, Italy},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://mlg-europe.github.io/papers/234.pdf},
	abstract = {Abstract Graph based models are used for tasks with increasing size and computational demands. We present a method for node classification that allows a user to precisely select the resolution at which the graph in question should be pretrained. Our method builds on an existing algorithm for pretraining on coarser graphs, {HARP}, which we extend in order to tune the effect of graph coarsening on the accuracy of node classification on a fine level. We present a novel way of refining the reduced graph in a targeted way based on the node classification confidence of particular nodes. This enhancement provides sufficient detail where needed, while collapsing structures where per-node information is not necessary for sufficient node classification accuracy. Hence, the method provides a meta-model for enhancing graph embedding models such as node2vec. We apply it to several datasets and discuss the differing behaviour on each of them in the context of their properties.},
	eventtitle = {20th International Workshop on Mining and Learning with Graphs},
	author = {Dědič, Marek and Bajer, Lukáš and Procházka, Pavel and Holeňa, Martin},
	urldate = {2023-09-22},
	date = {2023-09-22},
	langid = {english},
}

@inproceedings{prochazka_scalable_2022,
	location = {Zuberec, Slovakia},
	title = {Scalable Graph Size Reduction for Efficient {GNN} Application},
	volume = {3226},
	rights = {All rights reserved},
	url = {http://ceur-ws.org/Vol-3226/#paper9},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Graph neural networks ({GNN}) present a dominant framework for representation learning on graphs for the past several years. The main strength of {GNNs} lies in the fact that they can simultaneously learn from both node related attributes and relations between nodes, represented by edges. In tasks leading to large graphs, {GNN} often requires significant computational resources to achieve its superior performance. In order to reduce the computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple scalable task-aware graph preprocessing procedure allowing us to obtain a reduced graph such as {GNN} achieves a given desired performance on the downstream task. In addition, the proposed preprocessing allows for fitting the reduced graph and {GNN} into a given memory/computational resources. The proposed preprocessing is evaluated and compared with several reference scenarios on conventional {GNN} benchmark datasets.},
	eventtitle = {Information Technologies – Applications and Theory 2022},
	pages = {75--84},
	booktitle = {Proceedings of the 22nd Conference Information Technologies – Applications and Theory ({ITAT} 2022)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2022-09-30},
	date = {2022-09-30},
	langid = {english},
}

@inproceedings{gori_new_2005,
	title = {A new model for learning in graph domains},
	volume = {2},
	doi = {10.1109/IJCNN.2005.1555942},
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network ({GNN}), capable of directly processing graphs. {GNNs} extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for {GNNs} is proposed and some experiments are discussed which assess the properties of the model.},
	eventtitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	pages = {729--734 vol. 2},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	date = {2005-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Neural networks, Machine learning, Machine learning algorithms, Application software, data structures, Data structures, Encoding, Focusing, graph neural network, graph theory, graphical data structures, learning (artificial intelligence), learning algorithm, neural nets, Recurrent neural networks, recursive neural networks, Software engineering, Tree graphs},
	file = {IEEE Xplore Abstract Record:/home/madedic/Zotero/storage/33929VKF/1555942.html:text/html},
}

@inproceedings{dedic_balancing_2024,
	location = {Vienna, Austria},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://openreview.net/forum?id=DrHwIzz93C},
	abstract = {We present a method for graph node classification that allows a user to precisely select the resolution at which the graph in question should be simplified and through this provides a way of choosing a suitable point in the performance-complexity trade-off. The method is based on refining a reduced graph in a targeted way following the node classification confidence for particular nodes.},
	eventtitle = {The Twelfth International Conference on Learning Representations},
	booktitle = {The Second Tiny Papers Track at {ICLR} 2024},
	author = {Dědič, Marek and Bajer, Lukas and Prochazka, Pavel and Holena, Martin},
	urldate = {2024-04-18},
	date = {2024-05-11},
	langid = {english},
}
