
@inproceedings{prochazka_which_2023,
	location = {Tatranské Matliare, Slovakia},
	title = {Which Graph Properties Affect {GNN} Performance for a Given Downstream Task?},
	volume = {3498},
	rights = {All rights reserved},
	url = {https://ceur-ws.org/Vol-3498/#paper7},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Machine learning algorithms on graphs, in particular graph neural networks, became a popular framework for solving various tasks on graphs, attracting significant interest in the research community in recent years. As presented, however, these algorithms usually assume that the input graph is fixed and well-defined and do not consider the problem of constructing the graph for a given practical task. This work proposes a methodical way of linking graph properties with the performance of a {GNN} solving a given task on such graph via a surrogate regression model that is trained to predict the performance of the {GNN} from the properties of the graph dataset. Furthermore, the {GNN} model hyper-parameters are optionally added as additional features of the surrogate model and it is shown that this technique can be used to solve the practical problem of hyper-parameter tuning. We experimentally evaluate the importance of graph properties as features of the surrogate model with regards to the node classification task for several common graph datasets and discuss how these results can be used for graph composition tailored to the given task. Finally, our experiments indicate a significant gain in the proposed hyper-parameter tuning method compared to the reference grid-search method.},
	eventtitle = {Information Technologies – Applications and Theory 2023},
	pages = {58--66},
	booktitle = {Proceedings of the 23nd Conference Information Technologies – Applications and Theory ({ITAT} 2023)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2023-09-30},
	date = {2023-10-07},
	langid = {english},
}

@misc{pevny_nested_2020,
	title = {Nested Multiple Instance Learning in Modelling of {HTTP} network traffic},
	rights = {All rights reserved},
	url = {http://arxiv.org/abs/2002.04059},
	doi = {10.48550/arXiv.2002.04059},
	abstract = {In many interesting cases, the application of machine learning is hindered by data having a complicated structure stimulated by a structured file-formats like {JSONs}, {XMLs}, or {ProtoBuffers}, which is non-trivial to convert to a vector / matrix. Moreover, since the structure frequently carries a semantic meaning, reflecting it in the machine learning model should improve the accuracy but more importantly it facilitates the explanation of decisions and the model. This paper demonstrates on the identification of infected computers in the computer network from their {HTTP} traffic, how to achieve this reflection using recent progress in multiple-instance learning. The proposed model is compared to complementary approaches from the prior art, the first relying on human-designed features and the second on automatically learned features through convolution neural networks. In a challenging scenario measuring accuracy only on unseen domains/malware families, the proposed model is superior to the prior art while providing a valuable feedback to the security researchers. We believe that the proposed framework will found applications elsewhere even beyond the field of security.},
	number = {{arXiv}:2002.04059},
	publisher = {{arXiv}},
	author = {Pevny, Tomas and Dedic, Marek},
	urldate = {2023-10-11},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {2002.04059 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{dedic_balancing_2023,
	location = {Torino, Italy},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://mlg-europe.github.io/2023/papers/234.pdf},
	abstract = {Abstract Graph based models are used for tasks with increasing size and computational demands. We present a method for node classification that allows a user to precisely select the resolution at which the graph in question should be pretrained. Our method builds on an existing algorithm for pretraining on coarser graphs, {HARP}, which we extend in order to tune the effect of graph coarsening on the accuracy of node classification on a fine level. We present a novel way of refining the reduced graph in a targeted way based on the node classification confidence of particular nodes. This enhancement provides sufficient detail where needed, while collapsing structures where per-node information is not necessary for sufficient node classification accuracy. Hence, the method provides a meta-model for enhancing graph embedding models such as node2vec. We apply it to several datasets and discuss the differing behaviour on each of them in the context of their properties.},
	eventtitle = {20th International Workshop on Mining and Learning with Graphs},
	booktitle = {20th International Workshop on Mining and Learning with Graphs @{ECMLPKDD} 2023},
	author = {Dědič, Marek and Bajer, Lukáš and Procházka, Pavel and Holeňa, Martin},
	urldate = {2023-09-22},
	date = {2023-09-22},
	langid = {english},
}

@inproceedings{dedic_loss_2020,
	location = {Oravská Lesná, Slovakia},
	title = {Loss Functions for Clustering in Multi-instance Learning},
	volume = {2718},
	url = {http://ceur-ws.org/Vol-2718/#paper05},
	abstract = {Multi-instance learning belongs to one of recently fast developing areas of machine learning. It is a supervised learning method and this paper reports research into its unsupervised counterpart, multi-instance clustering. Whereas traditional clustering clusters points, multi-instance clustering clusters bags, i.e. multisets of points or of other kinds of objects. The paper focuses on the problem of loss functions for clustering. Three sophisticated loss functions used for clustering of points, contrastive predictive coding, triplet loss and magnet loss, are elaborated for multi-instance clustering. Finally, they are compared on 18 benchmark datasets, as well as on a real-world dataset.},
	eventtitle = {Information Technologies - Applications and Theory ({ITAT} 2020)},
	pages = {137--146},
	booktitle = {Proceedings of the 20th Conference Information Technologies - Applications and Theory ({ITAT} 2020)},
	publisher = {{CEUR}-{WS}.org},
	author = {Dědič, Marek and Pevný, Tomáš and Bajer, Lukáš and Holeňa, Martin},
	date = {2020-10-30},
	file = {Full Text:/home/madedic/Zotero/storage/UV9IPN8W/Dědič et al. - Loss Functions for Clustering in Multi-instance Le.pdf:application/pdf},
}

@article{yuan_explainability_2022,
	title = {Explainability in graph neural networks: A taxonomic survey},
	volume = {45},
	shorttitle = {Explainability in graph neural networks},
	pages = {5782--5799},
	number = {5},
	journaltitle = {{IEEE} transactions on pattern analysis and machine intelligence},
	author = {Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
	date = {2022},
	note = {Publisher: {IEEE}},
}

@inproceedings{ying_gnnexplainer_2019,
	title = {{GNNExplainer}: Generating Explanations for Graph Neural Networks},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html},
	shorttitle = {{GNNExplainer}},
	abstract = {Graph Neural Networks ({GNNs}) are a powerful tool for machine learning on graphs.{GNNs} combine node feature information with the graph structure by 
recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models,
and explaining predictions made by {GNNs} remains unsolved. Here
we propose {GNNExplainer}, the first general, model-agnostic approach for providing interpretable explanations for predictions of any {GNN}-based model on any graph-based machine learning task. Given an instance, {GNNExplainer} identifies a compact subgraph structure and a small subset of node features that have a crucial role in {GNN}'s prediction. 
Further, {GNNExplainer}  can generate consistent and concise explanations for an entire class of instances.
We formulate {GNNExplainer} as an optimization task that maximizes the mutual information between a {GNN}'s prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1\% on average. {GNNExplainer}  provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty {GNNs}.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
	urldate = {2023-08-30},
	date = {2019},
}

@inproceedings{dedic_adaptive_2022,
	location = {Grenoble, France},
	title = {Adaptive graph coarsening in the context of local graph quality},
	rights = {All rights reserved},
	url = {https://graphquality.github.io/rsc/art1.pdf},
	abstract = {Graph based models are used for tasks with increasing size and computational demands. We present a method for studying graph properties from the point of view of a downstream task. More precisely, the method allows a user to precisely select the resolution at which the graph in question should be coarsened. Our method builds on an existing algorithm for pretraining on coarser graphs, {HARP}. We extend both main parts of the algorithm in order to observe the effect of graph coarsening to model quality on a fine level. We present a general framework for graph coarsenings, providing two alternative algorithms based on graph diffusion convolution and evolutionary algorithms. Additionally, we present a novel way for un-coarsening the reduced graph in a targeted way based on the confidence of downstream classification for particular nodes. Together, these enhancements provide sufficient detail where needed, while collapsing structures where per-node information is not necessary for high model performance. Our method is a general meta-model for enhancing graph embedding models such as node2vec. We apply the method to several datasets and discuss the differing behaviour on each of them. Furthermore, we compare the proposed coarsening schemas.},
	eventtitle = {Data and Model Quality for Mining and Learning with Graphs: Methods and Open Challenges @{ECML}-{PKDD} 2022},
	booktitle = {Data and Model Quality for Mining and Learning with Graphs: Methods and Open Challenges @{ECML}-{PKDD} 2022},
	author = {Dědič, Marek and Bajer, Lukáš and Repický, Jakub and Procházka, Pavel and Holeňa, Martin},
	urldate = {2022-09-23},
	date = {2022-09-23},
	langid = {english},
}

@inproceedings{prochazka_scalable_2022,
	location = {Zuberec, Slovakia},
	title = {Scalable Graph Size Reduction for Efficient {GNN} Application},
	volume = {3226},
	rights = {All rights reserved},
	url = {http://ceur-ws.org/Vol-3226/#paper9},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Graph neural networks ({GNN}) present a dominant framework for representation learning on graphs for the past several years. The main strength of {GNNs} lies in the fact that they can simultaneously learn from both node related attributes and relations between nodes, represented by edges. In tasks leading to large graphs, {GNN} often requires significant computational resources to achieve its superior performance. In order to reduce the computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple scalable task-aware graph preprocessing procedure allowing us to obtain a reduced graph such as {GNN} achieves a given desired performance on the downstream task. In addition, the proposed preprocessing allows for fitting the reduced graph and {GNN} into a given memory/computational resources. The proposed preprocessing is evaluated and compared with several reference scenarios on conventional {GNN} benchmark datasets.},
	eventtitle = {Information Technologies – Applications and Theory 2022},
	pages = {75--84},
	booktitle = {Proceedings of the 22nd Conference Information Technologies – Applications and Theory ({ITAT} 2022)},
	publisher = {{CEUR}-{WS}.org},
	author = {Procházka, Pavel and Mareš, Michal and Dědič, Marek},
	urldate = {2022-09-30},
	date = {2022-09-30},
	langid = {english},
}

@inproceedings{grover_node2vec_2016,
	title = {node2vec: Scalable feature learning for networks},
	shorttitle = {node2vec},
	pages = {855--864},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Grover, Aditya and Leskovec, Jure},
	date = {2016},
	file = {Full Text:/home/madedic/Zotero/storage/Y8U8YZQI/PMC5108654.html:text/html;Snapshot:/home/madedic/Zotero/storage/ITX5DJ45/2939672.html:text/html},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: Online learning of social representations},
	shorttitle = {Deepwalk},
	pages = {701--710},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	date = {2014},
	file = {Full Text:/home/madedic/Zotero/storage/ISN68YG2/Perozzi et al. - 2014 - Deepwalk Online learning of social representation.pdf:application/pdf;Snapshot:/home/madedic/Zotero/storage/Y3TWLT7L/2623330.html:text/html},
}

@inproceedings{kipf_semi-supervised_2017,
	location = {Toulon, France},
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	url = {https://openreview.net/forum?id=SJU4ayYgl},
	abstract = {Semi-supervised classification with a {CNN} model for graphs. State-of-the-art results on a number of citation network datasets.},
	eventtitle = {International Conference on Learning Representations ({ICLR})},
	booktitle = {5th International Conference on Learning Representations, \{{ICLR}\} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher = {{OpenReview}.net},
	author = {Kipf, Thomas N. and Welling, Max},
	date = {2017-04-24},
	langid = {english},
}

@inproceedings{borisov_experimental_2021,
	location = {Heľpa, Slovakia},
	title = {Experimental Investigation of Neural and Weisfeiler-Lehman-Kernel Graph Representations for Downstream Classification},
	volume = {2962},
	rights = {All rights reserved},
	url = {http://ceur-ws.org/Vol-2962/#paper50},
	series = {{CEUR} Workshop Proceedings},
	abstract = {Graphs are one of the most ubiquitous kinds of data. However, data analysis methods have been developed primarily for numerical data, and to make use of them, graphs need to be represented as elements of some Euclidean space. An increasingly popular way of representing them in this way are graph neural networks ({GNNs}). Because data analysis applications typically require identical results for isomorphic graphs, the representations learned by {GNNs} also need to be invariant with respect to graph isomorphism. That motivated recent research into the possibilities of recognizing nonisomorphic pairs of graphs by {GNNs}, primarily based on the Weisfeiler-Lehman ({WL}) isomorphism test. This paper reports the results of a first experimental comparison of four variants of two important {GNNs} based on the {WL} test from the point of view of graph representation for downstream classification by means of a support vector machins ({SVM}). Those methods are compared not only with each other, but also with a recent generalization of the {WL} subtree kernel. For all {GNN} variants, two different representations are included in the comparison. The comparison revealed that the four considered representations of the same kind of {GNN} never significantly differ. On the other hand, there was always a statistically significant difference between representations originating from different kinds of {GNNs}, as well as between any representation originating from any of the considered {GNNs} and the representation originating from the generalized {WL} kernel.},
	eventtitle = {Information Technologies – Applications and Theory 2021},
	pages = {130--139},
	booktitle = {Proceedings of the 21st Conference Information Technologies – Applications and Theory ({ITAT} 2021)},
	publisher = {{CEUR}-{WS}.org},
	author = {Borisov, Sergej and Dědič, Marek and Holeňa, Martin},
	urldate = {2021-10-07},
	date = {2021-10-02},
	langid = {english},
	note = {{ISSN}: 1613-0073},
	file = {Full Text PDF:/home/madedic/Zotero/storage/Z5KJKGR4/Borisov et al. - 2021 - Experimental Investigation of Neural and Weisfeile.pdf:application/pdf},
}

@inproceedings{hamilton_inductive_2017,
	title = {Inductive representation learning on large graphs},
	pages = {1024--1034},
	booktitle = {Advances in neural information processing systems},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	date = {2017},
	file = {Full Text:/home/madedic/Zotero/storage/5Y9Q2N8U/Hamilton et al. - 2017 - Inductive representation learning on large graphs.pdf:application/pdf;Snapshot:/home/madedic/Zotero/storage/B8EPL4ZN/6703-inductive-representation-learning-on-large-graphs.html:text/html},
}

@article{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {https://openreview.net/forum?id=idpCdOWtqXd60},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity...},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2020-11-03},
	date = {2013-01-17},
	langid = {english},
	file = {Snapshot:/home/madedic/Zotero/storage/KERQU498/forum.html:text/html},
}

@online{kubara_machine_2020,
	title = {Machine Learning Tasks on Graphs},
	url = {https://towardsdatascience.com/machine-learning-tasks-on-graphs-7bc8f175119a},
	abstract = {Can We Divide It Into Supervised/Unsupervised Learning? It’s Not That Simple…},
	titleaddon = {Medium},
	author = {Kubara, Kacper},
	urldate = {2020-11-03},
	date = {2020-09-28},
	langid = {english},
	file = {Snapshot:/home/madedic/Zotero/storage/4IYMYT34/machine-learning-tasks-on-graphs-7bc8f175119a.html:text/html},
}

@inproceedings{gori_new_2005,
	title = {A new model for learning in graph domains},
	volume = {2},
	doi = {10.1109/IJCNN.2005.1555942},
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network ({GNN}), capable of directly processing graphs. {GNNs} extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for {GNNs} is proposed and some experiments are discussed which assess the properties of the model.},
	eventtitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	pages = {729--734 vol. 2},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	date = {2005-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Neural networks, Machine learning, Machine learning algorithms, Application software, data structures, Data structures, Encoding, Focusing, graph neural network, graph theory, graphical data structures, learning (artificial intelligence), learning algorithm, neural nets, Recurrent neural networks, recursive neural networks, Software engineering, Tree graphs},
	file = {IEEE Xplore Abstract Record:/home/madedic/Zotero/storage/33929VKF/1555942.html:text/html},
}

@article{dietterich_solving_1997,
	title = {Solving the multiple instance problem with axis-parallel rectangles},
	volume = {89},
	issn = {0004-3702},
	doi = {10.1016/S0004-3702(96)00034-3},
	abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
	pages = {31--71},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Dietterich, Thomas G. and Lathrop, Richard H. and Lozano-Pérez, Tomás},
	urldate = {2017-05-31},
	date = {1997-01-01},
	keywords = {Drug design, Machine learning, Structure-activity relationships},
}

@inproceedings{dedic_balancing_2024,
	location = {Vienna, Austria},
	title = {Balancing performance and complexity with adaptive graph coarsening},
	rights = {All rights reserved},
	url = {https://openreview.net/forum?id=DrHwIzz93C},
	abstract = {We present a method for graph node classification that allows a user to precisely select the resolution at which the graph in question should be simplified and through this provides a way of choosing a suitable point in the performance-complexity trade-off. The method is based on refining a reduced graph in a targeted way following the node classification confidence for particular nodes.},
	eventtitle = {The Twelfth International Conference on Learning Representations},
	booktitle = {The Second Tiny Papers Track at {ICLR} 2024},
	author = {Dědič, Marek and Bajer, Lukas and Prochazka, Pavel and Holena, Martin},
	urldate = {2024-04-18},
	date = {2024-05-11},
	langid = {english},
}

@inproceedings{prochazka_convolutional_2024,
	location = {Vilnius, Lithuania},
	title = {Convolutional Signal Propagation: A Simple Scalable Algorithm for Hypergraphs},
	rights = {All rights reserved},
	abstract = {Last decade has seen the emergence of numerous methods for learning on graphs, particularly Graph Neural Networks ({GNNs}). These methods, however, are often not directly applicable to more complex structures like bipartite graphs (equivalent to hypergraphs), which represent interactions among two entity types (e.g. a user liking a movie). This paper proposes Convolutional Signal Propagation ({CSP}), a non-parametric simple and scalable method that natively operates on bipartite graphs (hypergraphs) and can be implemented with just a few lines of code. After defining {CSP}, we demonstrate its relationship with well-established methods like label propagation, Naive Bayes, and Hypergraph Convolutional Networks. We evaluate {CSP} against several reference methods on real-world datasets from multiple domains, focusing on retrieval and classification tasks. Our results show that {CSP} offers competitive performance while maintaining low computational complexity, making it an ideal first choice as a baseline for hypergraph node classification and retrieval. Moreover, despite operating on hypergraphs, {CSP} achieves good results in tasks typically not associated with hypergraphs, such as natural language processing.},
	eventtitle = {21st International Workshop on Mining and Learning with Graphs},
	booktitle = {21st International Workshop on Mining and Learning with Graphs @{ECMLPKDD} 2024},
	author = {Procházka, Pavel and Dědič, Marek and Bajer, Lukáš},
	date = {2024-09-09},
	langid = {english},
}

@book{vapnik_nature_1995,
	location = {New York, {NY}},
	title = {The Nature of Statistical Learning Theory},
	rights = {http://www.springer.com/tdm},
	isbn = {978-1-4757-2440-0},
	url = {http://link.springer.com/10.1007/978-1-4757-2440-0},
	publisher = {Springer},
	author = {Vapnik, Vladimir N.},
	urldate = {2024-08-24},
	date = {1995},
	langid = {english},
	doi = {10.1007/978-1-4757-2440-0},
	keywords = {algorithms, boundary element method, construction, controlling, convergence, function, functional, learning, learning algorithm, learning theory, model, proof, Statistica, statistical theory, statistics},
	file = {Full Text:/home/madedic/Zotero/storage/3D37FJ2B/Vapnik - 1995 - The Nature of Statistical Learning Theory.pdf:application/pdf},
}

@inproceedings{gilmer_neural_2017,
	location = {Sydney, {NSW}, Australia},
	title = {Neural message passing for Quantum chemistry},
	series = {{ICML}'17},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks ({MPNNs}) and explore additional novel variations within this framework. Using {MPNNs} we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	pages = {1263--1272},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	publisher = {{JMLR}.org},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	urldate = {2024-08-26},
	date = {2017-08-06},
	file = {Full Text PDF:/home/madedic/Zotero/storage/KM28LB45/Gilmer et al. - 2017 - Neural message passing for Quantum chemistry.pdf:application/pdf},
}

@inproceedings{selvaraju_grad-cam_2017,
	title = {Grad-{CAM}: Visual Explanations From Deep Networks via Gradient-Based Localization},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html},
	shorttitle = {Grad-{CAM}},
	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
	pages = {618--626},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2024-08-26},
	date = {2017},
	file = {Full Text PDF:/home/madedic/Zotero/storage/LZ2R4K73/Selvaraju et al. - 2017 - Grad-CAM Visual Explanations From Deep Networks via Gradient-Based Localization.pdf:application/pdf},
}

@article{huang_graphlime_2023,
	title = {{GraphLIME}: Local Interpretable Model Explanations for Graph Neural Networks},
	volume = {35},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/9811416},
	doi = {10.1109/TKDE.2022.3187455},
	shorttitle = {{GraphLIME}},
	abstract = {Recently, graph neural networks ({GNN}) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. However, explaining the effectiveness of {GNN} models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose {GraphLIME}, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion ({HSIC}) Lasso, which is a nonlinear feature selection method. {GraphLIME} is a generic {GNN}-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. Through experiments on two real-world datasets, the explanations of {GraphLIME} are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.},
	pages = {6968--6972},
	number = {7},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Huang, Qiang and Yamada, Makoto and Tian, Yuan and Singh, Dinesh and Chang, Yi},
	urldate = {2024-08-26},
	date = {2023-07},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Computational modeling, Data models, explanation, Feature extraction, Graph neural networks, interpretability, Kernel, Mathematical models, Predictive models, Toy manufacturing industry},
	file = {IEEE Xplore Abstract Record:/home/madedic/Zotero/storage/9YUZMA5I/9811416.html:text/html;Submitted Version:/home/madedic/Zotero/storage/W5HC4JHC/Huang et al. - 2023 - GraphLIME Local Interpretable Model Explanations for Graph Neural Networks.pdf:application/pdf},
}

@inproceedings{ribeiro_why_2016,
	location = {San Francisco California {USA}},
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	isbn = {978-1-4503-4232-2},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	shorttitle = {"Why Should I Trust You?},
	eventtitle = {{KDD} '16: The 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {1135--1144},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2024-08-26},
	date = {2016-08-13},
	langid = {english},
}

@article{yamada_high-dimensional_2014,
	title = {High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso},
	volume = {26},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00537},
	doi = {10.1162/NECO_a_00537},
	abstract = {The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this letter, we consider a feature-wise kernelized Lasso for capturing nonlinear input-output dependency. We first show that with particular choices of kernel functions, nonredundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures such as the Hilbert-Schmidt independence criterion. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments for classification and regression with thousands of features.},
	pages = {185--207},
	number = {1},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Yamada, Makoto and Jitkrittum, Wittawat and Sigal, Leonid and Xing, Eric P. and Sugiyama, Masashi},
	urldate = {2024-08-26},
	date = {2014-01-01},
	file = {Snapshot:/home/madedic/Zotero/storage/TW3RMD6Q/High-Dimensional-Feature-Selection-by-Feature-Wise.html:text/html;Submitted Version:/home/madedic/Zotero/storage/UBVDAB6D/Yamada et al. - 2014 - High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso.pdf:application/pdf},
}

@inproceedings{luo_parameterized_2020,
	title = {Parameterized Explainer for Graph Neural Network},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/e37b08dd3015330dcbb5d6663667b8b8-Abstract.html},
	abstract = {Despite recent progress in Graph Neural Networks ({GNNs}), explaining predictions made by {GNNs} remains a challenging open problem.
The leading method mainly addresses the local explanations (i.e., important subgraph structure and node features) to interpret why a {GNN} model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized for each instance. The unique explanation interpreting each instance
independently is not sufficient to provide a global understanding of the learned {GNN} model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, as it is designed for explaining a single instance, it is challenging to explain a set of instances naturally (e.g., graphs of a given class).
In this study, we address these key challenges and propose {PGExplainer}, a parameterized explainer for {GNNs}. {PGExplainer} adopts a deep neural network to parameterize the generation process of explanations, which enables {PGExplainer} a natural approach to multi-instance explanations. Compared to the existing work, {PGExplainer} has a better generalization power and can be utilized in an inductive setting easily. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7{\textbackslash}\% relative improvement in {AUC} on explaining graph classification over the leading baseline.},
	pages = {19620--19631},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Luo, Dongsheng and Cheng, Wei and Xu, Dongkuan and Yu, Wenchao and Zong, Bo and Chen, Haifeng and Zhang, Xiang},
	urldate = {2024-08-27},
	date = {2020},
	file = {Full Text PDF:/home/madedic/Zotero/storage/C4EJ6GJ4/Luo et al. - 2020 - Parameterized Explainer for Graph Neural Network.pdf:application/pdf},
}

@article{gilbert_random_1959,
	title = {Random Graphs},
	volume = {30},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full},
	doi = {10.1214/aoms/1177706098},
	abstract = {The Annals of Mathematical Statistics},
	pages = {1141--1144},
	number = {4},
	journaltitle = {The Annals of Mathematical Statistics},
	author = {Gilbert, E. N.},
	urldate = {2024-08-27},
	date = {1959-12},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Full Text PDF:/home/madedic/Zotero/storage/BNF2GA6K/Gilbert - 1959 - Random Graphs.pdf:application/pdf},
}
